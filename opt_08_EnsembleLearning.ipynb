{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <img src=\"./assets/course-icon.png\" style=\"height:50px;display:inline\"> Learning Methods of Deep Learning\n",
    "---\n",
    "\n",
    "create by Deepfinder\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <img src=\"https://img.icons8.com/bubbles/50/000000/checklist.png\" style=\"height:50px;display:inline\"> Agenda\n",
    "---\n",
    "\n",
    "1. 师徒相授：有监督学习（Supervised Learning）\n",
    "2. 见微知著：无监督学习（Un-supervised Learning）\n",
    "3. 无师自通：自监督学习（Self-supervised Learning）\n",
    "4. 以点带面：半监督学习（Semi-supervised learning）\n",
    "5. 明辨是非：对比学习（Contrastive Learning）\n",
    "6. 举一反三：迁移学习（Transfer Learning）\n",
    "7. 针锋相对：对抗学习（Adversarial Learning）\n",
    "8. **众志成城：集成学习(Ensemble Learning)**\n",
    "9. 殊途同归：联邦学习（Federated Learning）\n",
    "10. 百折不挠：强化学习（Reinforcement Learning）\n",
    "11. 求知若渴：主动学习（Active Learning）\n",
    "12. 万法归宗：元学习（Meta-Learning）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial 08 - 众志成城：集成学习(Ensemble Learning) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在机器学习和深度学习中，**单一模型的性能往往受到模型复杂度、数据质量以及训练方法等诸多因素的限制**。\n",
    "\n",
    "为了解决这些问题，集成学习（Ensemble Learning）提供了一种强大的方法，将多个模型的预测结果结合起来，从而提升模型的整体性能、鲁棒性和泛化能力。\n",
    "\n",
    "集成学习在传统机器学习中有着广泛应用，比如随机森林（Random Forest）和梯度提升树（Gradient Boosting Trees）。近年来，随着深度学习的快速发展，集成学习的思想也被成功引入到深度学习中，为解决复杂任务（如图像分类、自然语言处理等）带来了更优的表现。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <img src=\"https://img.icons8.com/dusk/64/000000/popular-topic.png\" style=\"height:50px;display:inline\">  集成学习的核心概念\n",
    "---\n",
    "\n",
    "\n",
    "集成学习的基本思想是通过组合多个基模型（Base Model）的预测结果，达到“众人拾柴火焰高”的效果。其核心包括以下几点：\n",
    "\n",
    "1. 基模型（Base Model）\n",
    "\n",
    "基模型是参与集成的单一模型。对于深度学习而言，基模型可以是卷积神经网络（CNN）、循环神经网络（RNN）等。\n",
    "\n",
    "2. 集成策略\n",
    "\n",
    "* Bagging（Bootstrap Aggregating）：通过对数据集进行重采样，训练多个独立的基模型（如随机森林）。\n",
    "* Boosting：通过加权学习，逐步优化每个基模型，使其在上一轮中表现较差的数据上改进（如 AdaBoost 和梯度提升）。\n",
    "* Stacking（堆叠泛化）：使用一个元学习器（Meta-Learner）结合多个基模型的输出。\n",
    "\n",
    "3. 投票机制\n",
    "\n",
    "* 硬投票：基于每个模型的分类结果进行多数投票。\n",
    "* 软投票：结合每个模型的预测概率，取加权平均或最高概率值。\n",
    "* 通过以上策略，集成学习能够有效减少单一模型的过拟合风险，同时利用不同模型的互补优势来提升整体性能。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <img src=\"https://img.icons8.com/dusk/64/000000/lego-head.png\" style=\"height:50px;display:inline\"> 深度学习中的集成学习\n",
    "--- \n",
    "\n",
    "结合深度学习，集成学习在以下几个方面具有独特的价值：\n",
    "\n",
    "* 模型平均：多个模型的预测结果通过简单平均或加权平均融合，能够缓解单个模型的预测偏差。\n",
    "* 模型堆叠：多个模型的输出作为特征输入，利用轻量级学习器（如逻辑回归或浅层神经网络）学习更好的预测规则。\n",
    "* 模型多样性：不同网络架构、训练策略和数据增强技术的使用，有助于提高集成模型的鲁棒性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "\n",
    "# 检查 GPU 是否可用\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 数据预处理\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "train_dataset = datasets.CIFAR10(root='datasets', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.CIFAR10(root='datasets', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# 定义简单的 CNN 模型\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(64 * 16 * 16, 128)  # 修正全连接层输入大小\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.pool(self.relu(self.conv2(x)))\n",
    "        x = x.view(x.size(0), -1)  # 展平为 [batch_size, features]\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# 定义元模型（神经网络）\n",
    "class MetaModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(MetaModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型训练函数\n",
    "def train_model(model, train_loader, epochs=10):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    model.to(device)  # 将模型移动到 GPU\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)  # 将数据移动到 GPU\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # 测试集评估\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)  # 将数据移动到 GPU\n",
    "\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    return correct / total\n",
    "\n",
    "# 获取基模型的输出\n",
    "def get_predictions(models, loader):\n",
    "    outputs_list = []\n",
    "    labels_list = []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images, labels = images.to(device), labels.to(device)  # 将数据移动到 GPU\n",
    "\n",
    "            # 获取所有基模型的输出\n",
    "            model_outputs = [model(images) for model in models]\n",
    "            outputs_list.append(torch.cat(model_outputs, dim=1))  # 将基模型的输出拼接在一起\n",
    "\n",
    "            labels_list.append(labels)  # 获取标签\n",
    "\n",
    "    return torch.cat(outputs_list, dim=0), torch.cat(labels_list, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 集成学习（Bagging）\n",
    "def bagging_ensemble(models, test_loader):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)  # 将数据移动到 GPU\n",
    "\n",
    "            # 获取所有基模型的预测结果\n",
    "            outputs = torch.stack([model(images) for model in models], dim=0)\n",
    "            # 对多个模型的输出取平均\n",
    "            avg_outputs = torch.mean(outputs, dim=0)\n",
    "            \n",
    "            _, predicted = torch.max(avg_outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boosting 的训练与集成\n",
    "def boosting_ensemble(models, train_loader, test_loader, num_epochs=10):\n",
    "    model_weights = np.ones(len(models))  # 初始化每个模型的权重为1\n",
    "    model_accuracies = []  # 保存每个模型的准确率\n",
    "\n",
    "    # 训练每个模型并计算其准确率\n",
    "    for model in models:\n",
    "        accuracy = train_model(model, train_loader, epochs=num_epochs)\n",
    "        model_accuracies.append(accuracy)\n",
    "\n",
    "    # 调整模型的权重，根据准确率调整\n",
    "    total_accuracy = np.sum(model_accuracies)\n",
    "    model_weights = model_accuracies / total_accuracy  # 根据准确率计算权重\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)  # 将数据移动到 GPU\n",
    "\n",
    "            # 获取所有基模型的预测结果\n",
    "            outputs = torch.stack([model(images) for model in models], dim=0)\n",
    "            # 将权重转换为 tensor 并确保其在相同设备上\n",
    "            weighted_outputs = torch.sum(outputs * torch.tensor(model_weights, device=device).view(-1, 1, 1), dim=0)\n",
    "            _, predicted = torch.max(weighted_outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stacking 集成方法\n",
    "def stacking_ensemble(models, meta_model, train_loader, test_loader, epochs=20):\n",
    "    # 获取训练集和测试集的基模型输出\n",
    "    X_train, y_train = get_predictions(models, train_loader)\n",
    "    X_test, y_test = get_predictions(models, test_loader)\n",
    "\n",
    "    # 对基模型的输出进行 softmax 归一化\n",
    "    X_train = torch.softmax(X_train, dim=1)\n",
    "    X_test = torch.softmax(X_test, dim=1)\n",
    "\n",
    "    # 训练元模型\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(meta_model.parameters(), lr=0.001, weight_decay=1e-4)  # 增加权重衰减正则化\n",
    "\n",
    "    meta_model.to(device)  # 将元模型移动到 GPU\n",
    "    X_train, y_train = X_train.to(device), y_train.to(device)\n",
    "    X_test, y_test = X_test.to(device), y_test.to(device)\n",
    "\n",
    "    # 划分训练集和验证集\n",
    "    dataset_size = X_train.size(0)\n",
    "    indices = torch.randperm(dataset_size)\n",
    "    split = int(dataset_size * 0.8)  # 80% 训练，20% 验证\n",
    "    train_indices, val_indices = indices[:split], indices[split:]\n",
    "\n",
    "    X_train_split, y_train_split = X_train[train_indices], y_train[train_indices]\n",
    "    X_val, y_val = X_train[val_indices], y_train[val_indices]\n",
    "\n",
    "    best_accuracy = 0.0\n",
    "    for epoch in range(epochs):\n",
    "        meta_model.train()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = meta_model(X_train_split)\n",
    "        loss = criterion(outputs, y_train_split)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # 验证集评估\n",
    "        meta_model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_outputs = meta_model(X_val)\n",
    "            _, val_predicted = torch.max(val_outputs, 1)\n",
    "            val_accuracy = (val_predicted == y_val).float().mean().item()\n",
    "\n",
    "        # 保存最佳模型\n",
    "        if val_accuracy > best_accuracy:\n",
    "            best_accuracy = val_accuracy\n",
    "            torch.save(meta_model.state_dict(), 'best_meta_model.pth')\n",
    "\n",
    "\n",
    "    # 加载最佳模型\n",
    "    meta_model.load_state_dict(torch.load('best_meta_model.pth', weights_only=True))\n",
    "\n",
    "    # 测试集评估\n",
    "    meta_model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = meta_model(X_test)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        accuracy = (predicted == y_test).float().mean().item()\n",
    "\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline model accuracy: 68.07%\n"
     ]
    }
   ],
   "source": [
    "# 基线模型：训练一个单一的网络\n",
    "def baseline_model(train_loader, test_loader):\n",
    "    model = SimpleCNN().to(device)\n",
    "    accuracy = train_model(model, train_loader)\n",
    "    return accuracy\n",
    "\n",
    "# 训练基线模型\n",
    "baseline_accuracy = baseline_model(train_loader, test_loader)\n",
    "print(f\"Baseline model accuracy: {baseline_accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging accuracy: 73.26%\n",
      "Boosting accuracy: 73.35%\n",
      "Stacking accuracy: 71.43%\n"
     ]
    }
   ],
   "source": [
    "# 主程序\n",
    "num_models = 3  # 假设我们训练3个模型\n",
    "models = [SimpleCNN() for _ in range(num_models)]\n",
    "\n",
    "# 训练所有模型（Bagging 和 Boosting 需要）\n",
    "for model in models:\n",
    "    train_model(model, train_loader, epochs=5)\n",
    "\n",
    "# 使用 Bagging 方法进行模型集成\n",
    "bagging_accuracy = bagging_ensemble(models, test_loader)\n",
    "print(f\"Bagging accuracy: {bagging_accuracy * 100:.2f}%\")\n",
    "\n",
    "# 使用 Boosting 方法进行模型训练\n",
    "boosting_accuracy = boosting_ensemble(models, train_loader, test_loader, num_epochs=5)\n",
    "print(f\"Boosting accuracy: {boosting_accuracy * 100:.2f}%\")\n",
    "\n",
    "# 使用 Stacking 方法进行模型集成\n",
    "input_size = 10 * num_models  # 每个基模型输出10个类别，拼接后的输入大小\n",
    "meta_model = MetaModel(input_size, hidden_size=64, output_size=10)  # 使用神经网络作为元模型\n",
    "stacking_accuracy = stacking_ensemble(models, meta_model, train_loader, test_loader, epochs=100)\n",
    "print(f\"Stacking accuracy: {stacking_accuracy * 100:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
