{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <img src=\"./assets/course-icon.png\" style=\"height:50px;display:inline\"> Learning Methods of Deep Learning\n",
    "---\n",
    "\n",
    "create by Deepfinder\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <img src=\"https://img.icons8.com/bubbles/50/000000/checklist.png\" style=\"height:50px;display:inline\"> Agenda\n",
    "---\n",
    "\n",
    "1. å¸ˆå¾’ç›¸æˆï¼šæœ‰ç›‘ç£å­¦ä¹ ï¼ˆSupervised Learningï¼‰\n",
    "2. è§å¾®çŸ¥è‘—ï¼šæ— ç›‘ç£å­¦ä¹ ï¼ˆUn-supervised Learningï¼‰\n",
    "3. **æ— å¸ˆè‡ªé€šï¼šè‡ªç›‘ç£å­¦ä¹ ï¼ˆSelf-supervised Learningï¼‰**\n",
    "4. ä»¥ç‚¹å¸¦é¢ï¼šåŠç›‘ç£å­¦ä¹ ï¼ˆSemi-supervised learningï¼‰\n",
    "5. æ˜è¾¨æ˜¯éï¼šå¯¹æ¯”å­¦ä¹ ï¼ˆContrastive Learningï¼‰\n",
    "6. ä¸¾ä¸€åä¸‰ï¼šè¿ç§»å­¦ä¹ ï¼ˆTransfer Learningï¼‰\n",
    "7. é’ˆé”‹ç›¸å¯¹ï¼šå¯¹æŠ—å­¦ä¹ ï¼ˆAdversarial Learningï¼‰\n",
    "8. ä¼—å¿—æˆåŸï¼šé›†æˆå­¦ä¹ (Ensemble Learning) \n",
    "9. æ®Šé€”åŒå½’ï¼šè”é‚¦å­¦ä¹ ï¼ˆFederated Learningï¼‰\n",
    "10. ç™¾æŠ˜ä¸æŒ ï¼šå¼ºåŒ–å­¦ä¹ ï¼ˆReinforcement Learningï¼‰\n",
    "11. æ±‚çŸ¥è‹¥æ¸´ï¼šä¸»åŠ¨å­¦ä¹ ï¼ˆActive Learningï¼‰\n",
    "12. ä¸‡æ³•å½’å®—ï¼šå…ƒå­¦ä¹ ï¼ˆMeta-Learningï¼‰"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Tutorial 03 - æ— å¸ˆè‡ªé€šï¼šè‡ªç›‘ç£å­¦ä¹ ï¼ˆSelf-supervised Learningï¼‰\n",
    "\n",
    "## <img src=\"https://img.icons8.com/cute-clipart/64/000000/task.png\" style=\"height:50px;display:inline\"> è‡ªç›‘ç£å­¦ä¹ \n",
    "---\n",
    "* ä¸€ç§æ— ç›‘ç£å­¦ä¹ çš„ç‰ˆæœ¬ï¼Œå…¶ä¸­**æ•°æ®æä¾›ç›‘ç£**ã€‚\n",
    "* **æƒ³æ³•**ï¼šä¿ç•™éƒ¨åˆ†æ•°æ®ï¼Œç„¶åè®©ç¥ç»ç½‘ç»œæ ¹æ®å‰©ä½™éƒ¨åˆ†è¿›è¡Œé¢„æµ‹ã€‚\n",
    "\n",
    "### <img src=\"https://img.icons8.com/external-wanicon-lineal-color-wanicon/64/null/external-mask-brazilian-carnival-wanicon-lineal-color-wanicon.png\" style=\"height:50px;display:inline\"> Masked Autoencoders\n",
    "\n",
    "* <a href=\"https://arxiv.org/abs/2111.06377\">Masked Autoencoders Are Scalable Vision Learners, He et al. 2021.</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<center><img src=\"./assets/MAE.png\" style=\"height:250px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    " Masked Autoencoders (MAE) çš„åŸºæœ¬å·¥ä½œåŸç†ï¼š\n",
    "\n",
    "**æ•°æ®å¤„ç†é˜¶æ®µ**ï¼š\n",
    "å¯¹è¾“å…¥çš„å›¾åƒåˆ†å‰²æˆè®¸å¤šå°çš„å›¾åƒå—ï¼ˆpatchesï¼‰ã€‚\n",
    "éšæœºé€‰æ‹©ä¸€éƒ¨åˆ†å›¾åƒå—ï¼ˆé€šå¸¸æ˜¯ 75% çš„å›¾åƒå—ï¼‰è¿›è¡Œé®æ©ï¼ˆmaskï¼‰ï¼Œå³ä»è¾“å…¥ä¸­ç§»é™¤è¿™äº›å—ã€‚\n",
    "å‰©ä¸‹æœªé®æ©çš„å›¾åƒå—è¢«é€å…¥ç¼–ç å™¨ï¼ˆencoderï¼‰è¿›è¡Œç‰¹å¾æå–ã€‚\n",
    "\n",
    "**æ©ç ä»¤ç‰Œ** (Mask Tokens)ï¼š\n",
    "åœ¨ç¼–ç å™¨ä¹‹åï¼Œå°†æ©ç å—çš„ä½ç½®ç”¨ç‰¹æ®Šçš„â€œæ©ç ä»¤ç‰Œâ€æ¥è¡¥å……ã€‚æ©ç ä»¤ç‰Œæ˜¯ç”¨äºå¡«å……è¢«é®æ©çš„å›¾åƒå—ä½ç½®çš„ç‰¹æ®Šæ ‡è®°ã€‚å› ä¸ºè§£ç å™¨éœ€è¦å®Œæ•´çš„å›¾åƒå—åºåˆ—ï¼ˆåŒ…å«æœªé®æ©çš„å—å’Œé®æ©çš„å—ï¼‰æ¥é‡å»ºåŸå§‹å›¾åƒï¼Œæ‰€ä»¥æ©ç ä»¤ç‰Œåœ¨é®æ©å—çš„ä½ç½®ä¸Šèµ·åˆ°äº†å ä½çš„ä½œç”¨ã€‚é€šè¿‡è§£ç å™¨ï¼ˆdecoderï¼‰å¯¹ç¼–ç åçš„å›¾åƒå—å’Œæ©ç ä»¤ç‰Œè¿›è¡Œå¤„ç†ï¼Œå°è¯•é‡å»ºåŸå§‹å›¾åƒçš„åƒç´ ã€‚\n",
    "\n",
    "ç®€å•è¯´ï¼Œæ©ç ä»¤ç‰Œä½¿è§£ç å™¨èƒ½å¤ŸåŒºåˆ†å“ªäº›éƒ¨åˆ†æ˜¯å·²çŸ¥ä¿¡æ¯ï¼ˆç¼–ç å™¨æä¾›çš„æœªé®æ©å—ï¼‰ï¼Œå“ªäº›æ˜¯éœ€è¦é¢„æµ‹çš„æœªçŸ¥ä¿¡æ¯ï¼ˆç”±æ©ç å—çš„ä½ç½®æŒ‡ç¤ºï¼‰ã€‚è¿™æœ‰æ•ˆåœ°å¸®åŠ©è§£ç å™¨åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¸“æ³¨äºé‡å»ºè¢«é®æ©çš„éƒ¨åˆ†ã€‚\n",
    "\n",
    "**ä½¿ç”¨é˜¶æ®µ**ï¼š\n",
    "é¢„è®­ç»ƒå®Œæˆåï¼Œè§£ç å™¨è¢«ä¸¢å¼ƒï¼Œåªæœ‰ç¼–ç å™¨è¢«ä¿ç•™ã€‚\n",
    "å¯¹äºåç»­ä»»åŠ¡ï¼ˆå¦‚å›¾åƒåˆ†ç±»æˆ–ç›®æ ‡è¯†åˆ«ï¼‰ï¼Œç¼–ç å™¨æ¥æ”¶å®Œæ•´çš„æœªé®æ©å›¾åƒå—ä½œä¸ºè¾“å…¥ã€‚\n",
    "è¿™ç§æ–¹æ³•çš„æ ¸å¿ƒæ€æƒ³æ˜¯é€šè¿‡é¢„æµ‹è¢«é®æ©çš„éƒ¨åˆ†æ¥è®©æ¨¡å‹å­¦ä¹ æ›´å¥½çš„å›¾åƒè¡¨ç¤ºã€‚é¢„è®­ç»ƒé˜¶æ®µç±»ä¼¼äºè‡ªç›‘ç£å­¦ä¹ ï¼Œé€šè¿‡å¯¹é®æ©éƒ¨åˆ†çš„é‡å»ºæ¥æå‡ç¼–ç å™¨çš„ç‰¹å¾æå–èƒ½åŠ›ã€‚\n",
    "\n",
    "* ä»£ç ï¼š\n",
    "* HuggingFaceï¼š<a href=\"https://huggingface.co/docs/transformers/model_doc/vit_mae\">ViTMAE</a>\n",
    "* GitHubï¼š<a href=\"https://github.com/facebookresearch/mae\">å®˜æ–¹ PyTorch å®ç° (FAIR)</a>ã€<a href=\"https://github.com/EdisonLeeeee/Awesome-Masked-Autoencoders\">è¶…èµ MAE æ¨¡å‹</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Masked Language Model (MLM)**\n",
    "\n",
    "æ©ç è¯­è¨€æ¨¡å‹æ˜¯ BERT çš„å…³é”®è®­ç»ƒæ–¹å¼ï¼Œæ—¨åœ¨é€šè¿‡æ©ç éƒ¨åˆ†è¾“å…¥è¯æ±‡æ¥å­¦ä¹ ä¸Šä¸‹æ–‡è¯­ä¹‰ã€‚\n",
    "\n",
    "* è®­ç»ƒè¿‡ç¨‹ï¼š\n",
    "\n",
    "æ©ç éšæœºè¯ï¼š\n",
    "å¯¹è¾“å…¥æ–‡æœ¬ä¸­çš„è¯éšæœºé€‰æ‹© 15% è¿›è¡Œå¤„ç†ï¼š\n",
    "80% çš„æ¦‚ç‡ç”¨ [MASK] æ›¿æ¢ï¼ˆå¦‚ \"apple\" â†’ \"[MASK]\"ï¼‰ã€‚\n",
    "10% çš„æ¦‚ç‡æ›¿æ¢ä¸ºéšæœºè¯ï¼ˆå¦‚ \"apple\" â†’ \"orange\"ï¼‰ã€‚\n",
    "10% çš„æ¦‚ç‡ä¿æŒåŸè¯ä¸å˜ï¼ˆå¦‚ \"apple\" â†’ \"apple\"ï¼‰ã€‚\n",
    "\n",
    "* ç›®æ ‡ï¼š\n",
    "æ¨¡å‹é€šè¿‡ä¸Šä¸‹æ–‡é¢„æµ‹è¢«æ©ç çš„è¯ã€‚\n",
    "\n",
    "* ä½œç”¨ï¼š\n",
    "é€šè¿‡åŒå‘ä¸Šä¸‹æ–‡å»ºæ¨¡ï¼Œè®©æ¨¡å‹ç†è§£å¥å­ä¸­æ¯ä¸ªè¯ä¸å‘¨å›´è¯çš„å…³ç³»ï¼Œä»è€Œå­¦ä¹ æ›´æ·±å±‚æ¬¡çš„è¯­ä¹‰è¡¨ç¤ºã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"./assets/bert_1.png\" style=\"height:50px\"></center>\n",
    "\n",
    "<center><img src=\"./assets/bert_ill_1.png\" style=\"height:350px\"></center>\n",
    "\n",
    "* <a href=\"https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html\">Image Source 1</a>, <a href=\"http://jalammar.github.io/illustrated-bert/\">Image Source 2</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Next Sentence Prediction (NSP)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"./assets/bert_2.png\" style=\"height:100px\"></center>\n",
    "<center><img src=\"./assets/bert_ill_2.png\" style=\"height:350px\"></center>\n",
    "\n",
    "* <a href=\"https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html\">Image Source 1</a>, <a href=\"http://jalammar.github.io/illustrated-bert/\">Image Source 2</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <img src=\"https://img.icons8.com/?size=100&id=91CnU00i6HLv&format=png&color=000000\" style=\"height:50px;display:inline\"> è‡ªç›‘ç£å’Œæ— ç›‘ç£çš„æ ¸å¿ƒåŒºåˆ«åœ¨å“ªé‡Œï¼Ÿ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arwin/anaconda3/envs/dt/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "<frozen importlib._bootstrap>:219: RuntimeWarning: pyarrow.lib.Tensor size changed, may indicate binary incompatibility. Expected 64 from C header, got 80 from PyObject\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset splits: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['review', 'sentiment'],\n",
      "        num_rows: 40000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['review', 'sentiment'],\n",
      "        num_rows: 10000\n",
      "    })\n",
      "})\n",
      "Train samples: 40000\n",
      "Test samples: 10000\n",
      "Train sample[0]: {'review': \"The film disappointed me for many reasons: first of all the depiction of a future which seemed at first realistic to me was well-built but did only feature a marginal role. Then, the story itself was a weak copy of Lost in Translation. The Middle-Eastern setting, man with family meets new girl overseas, karaoke bar, the camera movements and the imagery - all that was a very bad imitation of the excellent Lost in Translation which had also credibility. This movie tries to be something brilliant and cultural: it is not. I wonder why Tim Robbins even considered doing this film!? The female main actress is awful - did she play the precog in Minority Report? And why do you have to show the vagina in a movie like this? Lost in Translation didn't have to show excessive love scenes. R-Rated just for this? This movie isn't even worth watching it from a videostore!\", 'sentiment': 'negative'}\n",
      "Test sample[0]: {'review': 'Arguably the finest serial ever made(no argument here thus far) about Earthman Flash Gordon, Professor Zarkov, and beautiful Dale Arden traveling in a rocket ship to another universe to save the planet. Along the way, in spellbinding, spectacular, and action-packed chapters Flash and his friends along with new found friends such as Prince Barin, Prince Thun, and the awesome King Vultan pool their resources together to fight the evils and armies of the merciless Ming of Mongo and the jealous treachery of his daughter Priness Aura(now she\\'s a car!). This serial is not just a cut above most serials in terms of plot, acting, and budget - it is miles ahead in these areas. Produced by Universal Studios it has many former sets at its disposable like the laboratory set from The Bride of Frankenstein and the Opera House from The Phantom of the Opera just to name a few. The production values across the board are advanced, in my most humble opinion, for 1936. The costumes worn by many of these strange men and women are really creative and first-rate. We get hawk-men, shark men, lion men, high priests, creatures like dragons, octasacks, orangapoids, and tigrons(oh my!)and many, many other fantastic things. Are all of them believable and first-rate special effects? No way. But for 1936 most are very impressive. The musical score is awesome and the chapter beginnings are well-written, lengthy enough to revitalize viewer memories of the former chapter, and expertly scored. Director Frederick Stephani does a great job piecing everything together wonderfully and creating a worthy film for Alex Raymond\\'s phenom comic strip. Lastly, the acting is pretty good in this serial. All too often serials have either no names with no talent surrounding one or two former talents - here most everyone has some ability. Don\\'t get me wrong, this isn\\'t a Shakespeare troupe by any means, but Buster Crabbe does a workmanlike, likable job as Flash. He is ably aided by Jean Arden, Priscella Lawson, and the rest of the cast in general with two performers standing out. But before I get to those two let me add as another reviewer noted, it must have been amazing for this serial to get by the Hayes Office. I see more flesh on Flash and on Jean Rogers and Priscella Lawson than in movies decades later. The shorts Crabbe(and unfortunately for all of us Professor Zarkov((Frank Shannon)) wears are about as form-fitting a pair of shorts guys can wear. The girls are wearing mid drifts throughout and are absolutely beautiful Jean Rogers may have limited acting talent but she is a blonde bombshell. Lawson is also very sultry and sensuous and beautiful. But for me the two actors that make the serial are Charles Middleton as Ming: officious, sardonic, merciless, and fun. Middleton is a class act. Jack \"Tiny\" Lipson plays King Vultan: boisterous, rousing, hilarious - a symbol for pure joy in life and the every essence of hedonism. Lipson steals each and every scene he is in. The plot meanders here, there, and everywhere - but Flash Gordon is the penultimate serial, space opera, and the basis for loads of science fiction to follow. Excellent!', 'sentiment': 'positive'}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "def load_and_split_dataset(csv_file_path, test_size=0.2, seed=42):\n",
    "    \"\"\"\n",
    "    ä» CSV æ–‡ä»¶ä¸­åŠ è½½æ•°æ®ï¼Œå¹¶æ‹†åˆ†ä¸ºè®­ç»ƒé›†ä¸æµ‹è¯•é›†ã€‚\n",
    "    \n",
    "    å‚æ•°:\n",
    "    -------\n",
    "    csv_file_path : str\n",
    "        CSV æ–‡ä»¶è·¯å¾„\n",
    "    test_size : float\n",
    "        æµ‹è¯•é›†å æ¯” (é»˜è®¤ä¸º 0.2, å³ 20%)\n",
    "    seed : int\n",
    "        éšæœºç§å­\n",
    "    \n",
    "    è¿”å›:\n",
    "    -------\n",
    "    dataset_dict : DatasetDict\n",
    "        åŒ…å« 'train' ä¸ 'test' ä¸¤ä¸ªåˆ‡åˆ†çš„ DatasetDict å¯¹è±¡\n",
    "    \"\"\"\n",
    "    # 1. åŠ è½½ CSV æ–‡ä»¶ï¼ˆå…¶ä¸­ä¸€åˆ—åä¸º \"review\", å¦ä¸€åˆ—åä¸º \"sentiment\"ï¼‰\n",
    "    raw_dataset = load_dataset(\n",
    "        \"csv\", \n",
    "        data_files=csv_file_path\n",
    "    )  \n",
    "    # æ³¨æ„ï¼šæ­¤æ—¶ raw_dataset åªåŒ…å«ä¸€ä¸ªåä¸º \"train\" çš„åˆ‡åˆ†ã€‚\n",
    "    # å› ä¸ºé»˜è®¤æƒ…å†µä¸‹è¯»å–å•ä¸€æ–‡ä»¶ä¼šæ”¾åœ¨ \"train\" è¿™ä¸ªåˆ‡åˆ†ä¸‹ã€‚\n",
    "    # ä½ å¯ä»¥ç”¨ raw_dataset[\"train\"] æ¥è®¿é—®å…¨éƒ¨æ•°æ®ã€‚\n",
    "\n",
    "    # 2. æŠŠå…¨éƒ¨æ•°æ®æ‹†åˆ†æˆè®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼ˆ8:2ï¼‰\n",
    "    # ä½¿ç”¨ train_test_split å°†åŸå§‹ raw_dataset[\"train\"] åˆ‡åˆ†ä¸º 'train' å’Œ 'test'\n",
    "    dataset_dict = raw_dataset[\"train\"].train_test_split(\n",
    "        test_size=test_size,\n",
    "        shuffle=True,\n",
    "        seed=seed\n",
    "    )\n",
    "\n",
    "    # 3. æ‰“å°æ•°æ®å½¢æ€ä¸ç¤ºä¾‹\n",
    "    print(f\"Dataset splits: {dataset_dict}\")\n",
    "    print(f\"Train samples: {dataset_dict['train'].num_rows}\")\n",
    "    print(f\"Test samples: {dataset_dict['test'].num_rows}\")\n",
    "    \n",
    "    # æ‰“å°å‰ä¸¤è¡Œä½œä¸ºç¤ºä¾‹\n",
    "    print(\"Train sample[0]:\", dataset_dict[\"train\"][0])\n",
    "    print(\"Test sample[0]:\", dataset_dict[\"test\"][0])\n",
    "\n",
    "    # è¿”å›åŒ…å« 'train' å’Œ 'test' çš„ DatasetDict\n",
    "    return dataset_dict\n",
    "\n",
    "csv_path = \"datasets/imdb/IMDB Dataset.csv\"  # ä½ çš„ CSV æ–‡ä»¶è·¯å¾„\n",
    "dataset = load_and_split_dataset(\n",
    "    csv_file_path=csv_path,\n",
    "    test_size=0.2,\n",
    "    seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertForMaskedLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From ğŸ‘‰v4.50ğŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "Some weights of the model checkpoint at datasets/bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/home/arwin/anaconda3/envs/dt/lib/python3.8/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/home/arwin/anaconda3/envs/dt/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/home/arwin/anaconda3/envs/dt/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2500' max='2500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2500/2500 20:34, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.984600</td>\n",
       "      <td>1.933474</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arwin/anaconda3/envs/dt/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/home/arwin/anaconda3/envs/dt/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/arwin/anaconda3/envs/dt/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/home/arwin/anaconda3/envs/dt/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/arwin/anaconda3/envs/dt/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/home/arwin/anaconda3/envs/dt/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/arwin/anaconda3/envs/dt/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/home/arwin/anaconda3/envs/dt/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/arwin/anaconda3/envs/dt/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/home/arwin/anaconda3/envs/dt/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='625' max='625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [625/625 02:10]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.9387198686599731, 'eval_runtime': 130.2856, 'eval_samples_per_second': 76.754, 'eval_steps_per_second': 4.797, 'epoch': 1.0}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    BertTokenizer,\n",
    "    BertForMaskedLM,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "\n",
    "# 1. åŠ è½½ IMDB æ•°æ®é›†\n",
    "#    æ•°æ®é›†åŒ…å« \"train\" å’Œ \"test\" ä¸¤ä¸ªåˆ‡åˆ†ï¼Œæ¯æ¡æ•°æ®åŒ…å« \"text\" å’Œ \"label\" å­—æ®µã€‚\n",
    "imdb_dataset = dataset\n",
    "\n",
    "# 2. åˆå§‹åŒ–åˆ†è¯å™¨ï¼ˆTokenizerï¼‰\n",
    "tokenizer = BertTokenizer.from_pretrained(\"datasets/bert-base-uncased\")\n",
    "\n",
    "# 3. å®šä¹‰åˆ†è¯å‡½æ•°ï¼Œå¹¶å¯¹æ•°æ®é›†è¿›è¡Œåˆ†è¯ä¸æ•°å€¼åŒ–\n",
    "#    - `padding=\"max_length\"`: å°†å¥å­è¡¥åˆ°åŒæ ·é•¿åº¦\n",
    "#    - `truncation=True`    : è¶…è¿‡æŒ‡å®šé•¿åº¦ä¼šè¿›è¡Œæˆªæ–­\n",
    "#    - `max_length=128`     : ç»Ÿä¸€åˆ° 128 çš„åºåˆ—é•¿åº¦\n",
    "def tokenize_function(examples):\n",
    "    # æ³¨æ„åˆ—åç”¨ \"review\"\n",
    "    return tokenizer(\n",
    "        examples[\"review\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=128\n",
    "    )\n",
    "\n",
    "# remove_columns=[\"review\"] è¡¨ç¤ºå¤„ç†åå»æ‰åŸå§‹æ–‡æœ¬åˆ—ï¼Œåªä¿ç•™æ¨¡å‹æ‰€éœ€çš„å­—æ®µ\n",
    "tokenized_imdb = imdb_dataset.map(\n",
    "    tokenize_function, \n",
    "    batched=True, \n",
    "    remove_columns=[\"review\"]\n",
    ")\n",
    "\n",
    "# 4. å‡†å¤‡ DataCollator\n",
    "#    DataCollatorForLanguageModeling ä¼šè‡ªåŠ¨å¯¹ batch å†…çš„å¥å­è¿›è¡Œéšæœº Mask\n",
    "#    mlm_probability=0.15 è¡¨ç¤ºåœ¨ä¸€ä¸ªå¥å­ä¸­æœ‰ 15% çš„ Token è¢«éšæœº Maskã€‚\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=True,\n",
    "    mlm_probability=0.15\n",
    ")\n",
    "\n",
    "# 5. å®šä¹‰ BERT MLM æ¨¡å‹\n",
    "model = BertForMaskedLM.from_pretrained(\"datasets/bert-base-uncased\")\n",
    "\n",
    "# 6. è®­ç»ƒé…ç½®\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./mlm_imdb_bert\",      # æ¨¡å‹è¾“å‡ºè·¯å¾„\n",
    "    evaluation_strategy=\"epoch\",       # æ¯ä¸ª epoch ç»“æŸåè¿›è¡Œä¸€æ¬¡è¯„ä¼°\n",
    "    per_device_train_batch_size=8,     # è®­ç»ƒæ—¶æ¯å— GPU/CPU çš„ batch size\n",
    "    per_device_eval_batch_size=8,      # æµ‹è¯•æ—¶æ¯å— GPU/CPU çš„ batch size\n",
    "    num_train_epochs=1,                # æ¼”ç¤ºç”¨è®­ç»ƒè½®æ•°ï¼Œå¯æ ¹æ®éœ€è¦ä¿®æ”¹\n",
    "    logging_steps=100,                 # æ¯éš”å¤šå°‘æ­¥æ‰“å°æ—¥å¿—\n",
    "    save_steps=500                     # å¤šå°‘æ­¥ä¿å­˜ä¸€æ¬¡æ¨¡å‹\n",
    ")\n",
    "\n",
    "# 7. ç”¨ Trainer æ¥å°è£…è®­ç»ƒæµç¨‹\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_imdb[\"train\"],  # è®­ç»ƒé›†\n",
    "    eval_dataset=tokenized_imdb[\"test\"],    # æµ‹è¯•é›†\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "# 8. è¿›è¡Œè®­ç»ƒ\n",
    "trainer.train()\n",
    "\n",
    "# 9. è®­ç»ƒå®Œæˆåï¼Œå¯ä½¿ç”¨ trainer.evaluate() å¯¹æµ‹è¯•é›†è¿›è¡Œè¯„ä¼°\n",
    "eval_results = trainer.evaluate()\n",
    "print(eval_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <img src=\"https://img.icons8.com/dusk/64/000000/prize.png\" style=\"height:50px;display:inline\"> Credits\n",
    "---\n",
    "* Icons made by <a href=\"https://www.flaticon.com/authors/becris\" title=\"Becris\">Becris</a> from <a href=\"https://www.flaticon.com/\" title=\"Flaticon\">www.flaticon.com</a>\n",
    "* Icons from <a href=\"https://icons8.com/\">Icons8.com</a> - https://icons8.com\n",
    "* Datasets from <a href=\"https://www.kaggle.com/\">Kaggle</a> - https://www.kaggle.com/\n",
    "* <a href=\"https://machinelearningmastery.com/why-initialize-a-neural-network-with-random-weights/\">Jason Brownlee - Why Initialize a Neural Network with Random Weights?</a>\n",
    "* <a href=\"https://openai.com/blog/deep-double-descent/\">OpenAI - Deep Double Descent</a>\n",
    "* <a href=\"https://taldatech.github.io\">Tal Daniel</a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
